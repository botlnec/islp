<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">   
    <link rel="shortcut icon" href="../../../img/favicon.ico">

    <title>3.14 - Statistical Learning with Python</title>

    <link href="../../../css/bootstrap-custom.min.css" rel="stylesheet">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/font-hack/2.018/css/hack.min.css">
    <link href='//fonts.googleapis.com/css?family=PT+Sans:400,400italic,700,700italic&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href="../../../css/base.css" rel="stylesheet">
    <link href="../../../css/cinder.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/highlight.css">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script>
    <script>
    WebFont.load({
        google: {
            families: ['Open Sans', 'PT Sans']
        }
    });
    </script>

    
</head>

<body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->
            <a class="navbar-brand" href="../../..">Statistical Learning with Python</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                
                
                    <li >
                        <a href="../../..">Home</a>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">2 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../chapter2/exercise1/">2.1</a>
</li>

                        
                            
<li >
    <a href="../../chapter2/exercise2/">2.2</a>
</li>

                        
                            
<li >
    <a href="../../chapter2/exercise3/">2.3</a>
</li>

                        
                            
<li >
    <a href="../../chapter2/exercise4/">2.4</a>
</li>

                        
                            
<li >
    <a href="../../chapter2/exercise5/">2.5</a>
</li>

                        
                            
<li >
    <a href="../../chapter2/exercise6/">2.6</a>
</li>

                        
                            
<li >
    <a href="../../chapter2/exercise7/">2.7</a>
</li>

                        
                            
<li >
    <a href="../../chapter2/exercise8/">2.8</a>
</li>

                        
                            
<li >
    <a href="../../chapter2/exercise9/">2.9</a>
</li>

                        
                            
<li >
    <a href="../../chapter2/exercise10/">2.10</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">3 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../exercise1/">3.1</a>
</li>

                        
                            
<li >
    <a href="../exercise2/">3.2</a>
</li>

                        
                            
<li >
    <a href="../exercise3/">3.3</a>
</li>

                        
                            
<li >
    <a href="../exercise4/">3.4</a>
</li>

                        
                            
<li >
    <a href="../exercise5/">3.5</a>
</li>

                        
                            
<li >
    <a href="../exercise6/">3.6</a>
</li>

                        
                            
<li >
    <a href="../exercise7/">3.7</a>
</li>

                        
                            
<li >
    <a href="../exercise8/">3.8</a>
</li>

                        
                            
<li >
    <a href="../exercise9/">3.9</a>
</li>

                        
                            
<li >
    <a href="../exercise10/">3.10</a>
</li>

                        
                            
<li >
    <a href="../exercise11/">3.11</a>
</li>

                        
                            
<li >
    <a href="../exercise12/">3.12</a>
</li>

                        
                            
<li >
    <a href="../exercise13/">3.13</a>
</li>

                        
                            
<li class="active">
    <a href="./">3.14</a>
</li>

                        
                            
<li >
    <a href="../exercise15/">3.15</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">4 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../chapter4/exercise1/">4.1</a>
</li>

                        
                            
<li >
    <a href="../../chapter4/exercise2/">4.2</a>
</li>

                        
                            
<li >
    <a href="../../chapter4/exercise3/">4.3</a>
</li>

                        
                            
<li >
    <a href="../../chapter4/exercise4/">4.4</a>
</li>

                        
                            
<li >
    <a href="../../chapter4/exercise5/">4.5</a>
</li>

                        
                            
<li >
    <a href="../../chapter4/exercise6/">4.6</a>
</li>

                        
                            
<li >
    <a href="../../chapter4/exercise7/">4.7</a>
</li>

                        
                            
<li >
    <a href="../../chapter4/exercise8/">4.8</a>
</li>

                        
                            
<li >
    <a href="../../chapter4/exercise9/">4.9</a>
</li>

                        
                            
<li >
    <a href="../../chapter4/exercise10/">4.10</a>
</li>

                        
                            
<li >
    <a href="../../chapter4/exercise11/">4.11</a>
</li>

                        
                            
<li >
    <a href="../../chapter4/exercise12/">4.12</a>
</li>

                        
                            
<li >
    <a href="../../chapter4/exercise13/">4.13</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">5 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../chapter5/exercise1/">5.1</a>
</li>

                        
                            
<li >
    <a href="../../chapter5/exercise2/">5.2</a>
</li>

                        
                            
<li >
    <a href="../../chapter5/exercise3/">5.3</a>
</li>

                        
                            
<li >
    <a href="../../chapter5/exercise4/">5.4</a>
</li>

                        
                            
<li >
    <a href="../../chapter5/exercise5/">5.5</a>
</li>

                        
                            
<li >
    <a href="../../chapter5/exercise6/">5.6</a>
</li>

                        
                            
<li >
    <a href="../../chapter5/exercise7/">5.7</a>
</li>

                        
                            
<li >
    <a href="../../chapter5/exercise8/">5.8</a>
</li>

                        
                            
<li >
    <a href="../../chapter5/exercise9/">5.9</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">6 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../chapter6/exercise1/">6.1</a>
</li>

                        
                            
<li >
    <a href="../../chapter6/exercise2/">6.2</a>
</li>

                        
                            
<li >
    <a href="../../chapter6/exercise3/">6.3</a>
</li>

                        
                            
<li >
    <a href="../../chapter6/exercise4/">6.4</a>
</li>

                        
                            
<li >
    <a href="../../chapter6/exercise5/">6.5</a>
</li>

                        
                            
<li >
    <a href="../../chapter6/exercise6/">6.6</a>
</li>

                        
                            
<li >
    <a href="../../chapter6/exercise7/">6.7</a>
</li>

                        
                            
<li >
    <a href="../../chapter6/exercise8/">6.8</a>
</li>

                        
                            
<li >
    <a href="../../chapter6/exercise9/">6.9</a>
</li>

                        
                            
<li >
    <a href="../../chapter6/exercise10/">6.10</a>
</li>

                        
                            
<li >
    <a href="../../chapter6/exercise11/">6.11</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">7 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../chapter7/exercise1/">7.1</a>
</li>

                        
                            
<li >
    <a href="../../chapter7/exercise2/">7.2</a>
</li>

                        
                            
<li >
    <a href="../../chapter7/exercise3/">7.3</a>
</li>

                        
                            
<li >
    <a href="../../chapter7/exercise4/">7.4</a>
</li>

                        
                            
<li >
    <a href="../../chapter7/exercise5/">7.5</a>
</li>

                        
                            
<li >
    <a href="../../chapter7/exercise6/">7.6</a>
</li>

                        
                            
<li >
    <a href="../../chapter7/exercise7/">7.7</a>
</li>

                        
                            
<li >
    <a href="../../chapter7/exercise8/">7.8</a>
</li>

                        
                            
<li >
    <a href="../../chapter7/exercise9/">7.9</a>
</li>

                        
                            
<li >
    <a href="../../chapter7/exercise10/">7.10</a>
</li>

                        
                            
<li >
    <a href="../../chapter7/exercise11/">7.11</a>
</li>

                        
                            
<li >
    <a href="../../chapter7/exercise12/">7.12</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">8 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../chapter8/exercise1/">8.1</a>
</li>

                        
                            
<li >
    <a href="../../chapter8/exercise2/">8.2</a>
</li>

                        
                            
<li >
    <a href="../../chapter8/exercise3/">8.3</a>
</li>

                        
                            
<li >
    <a href="../../chapter8/exercise4/">8.4</a>
</li>

                        
                            
<li >
    <a href="../../chapter8/exercise5/">8.5</a>
</li>

                        
                            
<li >
    <a href="../../chapter8/exercise6/">8.6</a>
</li>

                        
                            
<li >
    <a href="../../chapter8/exercise7/">8.7</a>
</li>

                        
                            
<li >
    <a href="../../chapter8/exercise8/">8.8</a>
</li>

                        
                            
<li >
    <a href="../../chapter8/exercise9/">8.9</a>
</li>

                        
                            
<li >
    <a href="../../chapter8/exercise10/">8.10</a>
</li>

                        
                            
<li >
    <a href="../../chapter8/exercise11/">8.11</a>
</li>

                        
                            
<li >
    <a href="../../chapter8/exercise12/">8.12</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">9 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../chapter9/exercise1/">9.1</a>
</li>

                        
                            
<li >
    <a href="../../chapter9/exercise2/">9.2</a>
</li>

                        
                            
<li >
    <a href="../../chapter9/exercise3/">9.3</a>
</li>

                        
                            
<li >
    <a href="../../chapter9/exercise4/">9.4</a>
</li>

                        
                            
<li >
    <a href="../../chapter9/exercise5/">9.5</a>
</li>

                        
                            
<li >
    <a href="../../chapter9/exercise6/">9.6</a>
</li>

                        
                            
<li >
    <a href="../../chapter9/exercise7/">9.7</a>
</li>

                        
                            
<li >
    <a href="../../chapter9/exercise8/">9.8</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">10 <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../chapter10/exercise1/">10.1</a>
</li>

                        
                            
<li >
    <a href="../../chapter10/exercise2/">10.2</a>
</li>

                        
                            
<li >
    <a href="../../chapter10/exercise3/">10.3</a>
</li>

                        
                            
<li >
    <a href="../../chapter10/exercise4/">10.4</a>
</li>

                        
                            
<li >
    <a href="../../chapter10/exercise5/">10.5</a>
</li>

                        
                            
<li >
    <a href="../../chapter10/exercise6/">10.6</a>
</li>

                        
                            
<li >
    <a href="../../chapter10/exercise7/">10.7</a>
</li>

                        
                            
<li >
    <a href="../../chapter10/exercise8/">10.8</a>
</li>

                        
                            
<li >
    <a href="../../chapter10/exercise9/">10.9</a>
</li>

                        
                            
<li >
    <a href="../../chapter10/exercise10/">10.10</a>
</li>

                        
                            
<li >
    <a href="../../chapter10/exercise11/">10.11</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li >
                        <a href="../../../about/">About</a>
                    </li>
                
                
                </ul>
            

            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                        <i class="fa fa-search"></i> Search
                    </a>
                </li>
                
                    <li >
                        <a rel="next" href="../exercise13/">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../exercise15/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>
                
                
            </ul>
        </div>
    </div>
</div>

    <div class="container">
        
        <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="first-level active"><a href="#2do">2DO</a></li>
        
    
        <li class="first-level "><a href="#exercise-314">Exercise 3.14</a></li>
        
    
        <li class="first-level "><a href="#a">(a)</a></li>
        
    
        <li class="first-level "><a href="#b">(b)</a></li>
        
    
        <li class="first-level "><a href="#c">(c)</a></li>
        
    
        <li class="first-level "><a href="#d">(d)</a></li>
        
    
        <li class="first-level "><a href="#e">(e)</a></li>
        
    
        <li class="first-level "><a href="#f">(f)</a></li>
        
    
        <li class="first-level "><a href="#g">(g)</a></li>
        
            <li class="second-level"><a href="#models-analysis">Models analysis</a></li>
            
        
            <li class="second-level"><a href="#outliers-and-high-leverage-points-analysis">Outliers and high leverage points analysis</a></li>
            
        
            <li class="second-level"><a href="#references">References</a></li>
            
        
    
    </ul>
</div></div>
        <div class="col-md-9" role="main">

<h1 id="2do">2DO</h1>
<ul>
<li>Paraphrase text about outliers and high leverage points. It's just copy/paste</li>
<li>Check (g) carefully. I answered in a qualitative way. I believe it's correct but it's different from regular solutions.</li>
<li>JR: in (g) the analysis done was univariate, not bivariate as indicated, should be corrected either way.</li>
<li>JR: for me, all good, except revision of (g)</li>
</ul>
<h1 id="exercise-314">Exercise 3.14</h1>
<pre><code class="python">import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import statsmodels.api as sm #to use statsmodel
import statsmodels.formula.api as smf #to use statsmodel with R-style formulas
from statsmodels.stats import outliers_influence

%matplotlib inline
</code></pre>

<pre><code>/Users/disciplina/anaconda3/envs/islp/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.
  from pandas.core import datetools
</code></pre>
<h1 id="a">(a)</h1>
<pre><code class="python">np.random.seed(5) # Python and R random generators give different values
x1 = np.random.uniform(size=100)
x2 = 0.5 * x1 + np.random.normal(size=100) / 10
y = 2 + 2 * x1 + 0.3 * x2 + np.random.normal(size=100)

# http://stackoverflow.com/questions/22213298/creating-same-random-number-sequence-in-python-numpy-and-r
</code></pre>

<p>Model form:</p>
<span>\[
Y = \beta_0 + \beta_1  X_1 + \beta_2  X_2 + \epsilon = 2 + 2 X_1 + 0.3 X_2 + \epsilon.
\]</span><h1 id="b">(b)</h1>
<pre><code class="python"># get correlations
np.corrcoef(x1,x2)
</code></pre>

<pre><code>array([[ 1.        ,  0.81936924],
       [ 0.81936924,  1.        ]])
</code></pre>
<p>The correlation coefficient between <span>\(X_1\)</span> and <span>\(X_2\)</span> is <b>0.819</b>.</p>
<pre><code class="python"># draw scatterplot
plt.scatter(x1,x2);
</code></pre>

<p><img alt="png" src="../03_14_files/03_14_9_0.png" /></p>
<h1 id="c">(c)</h1>
<pre><code class="python"># define data
X = pd.DataFrame({'x1':x1, 'x2':x2})
X = sm.add_constant(X) # no constant is added by the model unless we're using formulas, so we have to add it

# create model
model = sm.OLS(y, X)

# fit regression model
results = model.fit()

# print results
results.summary()
</code></pre>

<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.444</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.433</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   38.74</td>
</tr>
<tr>
  <th>Date:</th>             <td>Thu, 30 Nov 2017</td> <th>  Prob (F-statistic):</th> <td>4.31e-13</td>
</tr>
<tr>
  <th>Time:</th>                 <td>20:25:36</td>     <th>  Log-Likelihood:    </th> <td> -123.67</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   253.3</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>   261.1</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>

<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td>    1.8158</td> <td>    0.162</td> <td>   11.231</td> <td> 0.000</td> <td>    1.495</td> <td>    2.137</td>
</tr>
<tr>
  <th>x1</th>    <td>    2.0758</td> <td>    0.488</td> <td>    4.257</td> <td> 0.000</td> <td>    1.108</td> <td>    3.044</td>
</tr>
<tr>
  <th>x2</th>    <td>    0.7584</td> <td>    0.817</td> <td>    0.929</td> <td> 0.355</td> <td>   -0.862</td> <td>    2.379</td>
</tr>
</table>

<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.718</td> <th>  Durbin-Watson:     </th> <td>   1.960</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.698</td> <th>  Jarque-Bera (JB):  </th> <td>   0.574</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.185</td> <th>  Prob(JB):          </th> <td>   0.750</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.981</td> <th>  Cond. No.          </th> <td>    12.5</td>
</tr>
</table>

<p>According to the results we have:</p>
<ul>
<li><span>\(\hat{\beta_0} = 1.8158\)</span> </li>
<li><span>\(\hat{\beta_1} = 2.0758\)</span> </li>
<li><span>\(\hat{\beta_2} = 0.7584\)</span> </li>
</ul>
<p>These values are estimators of the true coefficients, which have the following values:</p>
<ul>
<li><span>\(\beta_0 = 2\)</span></li>
<li><span>\(\beta_1 = 2\)</span></li>
<li><span>\(\beta_2 = 0.3\)</span></li>
</ul>
<p>As we can see, there are some differences between the coefficients, especially in the case of <span>\(\hat{\beta_2}\)</span> (0.7584 vs. 0.3). </p>
<p><b> <span>\(H_0 : \beta_1 = 0\)</span> </b>. The rejection of the null hypothesis depends on the t-statistic (t). In the case of <span>\(\beta_1\)</span>, this value is high. If the t-statistics is high, the p-value will be low. The p-value is the probability of observing any value equal to |t| or larger (P&gt;|t|), assuming that the coefficient is zero. Thus, if the p-value is low we should <b>reject the null hypothesis and accept the alternative hypothesis</b>.</p>
<p><b> <span>\(H_0 : \beta_2 = 0\)</span> </b>. In this case the t-statistic is low (0.929) and the p-value is high (0.355). Accordingly, the <b>null hypothesis can't be rejected</b>.</p>
<p><b>Alternative solution</b></p>
<p>A different way to approximate the equation using R-style formula in StatsModel.</p>
<pre><code class="python"># define data
df = pd.DataFrame({'x1':x1, 'x2':x2, 'y':y}) # dataframe to be read like R; we don't need to add constant because we will use formulas

# create model
mod = smf.ols(formula='y ~ x1 + x2', data=df) # R-style command

# fit model
res = mod.fit()

# print results
print (res.summary())
</code></pre>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.444
Model:                            OLS   Adj. R-squared:                  0.433
Method:                 Least Squares   F-statistic:                     38.74
Date:                Thu, 30 Nov 2017   Prob (F-statistic):           4.31e-13
Time:                        20:25:38   Log-Likelihood:                -123.67
No. Observations:                 100   AIC:                             253.3
Df Residuals:                      97   BIC:                             261.1
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      1.8158      0.162     11.231      0.000       1.495       2.137
x1             2.0758      0.488      4.257      0.000       1.108       3.044
x2             0.7584      0.817      0.929      0.355      -0.862       2.379
==============================================================================
Omnibus:                        0.718   Durbin-Watson:                   1.960
Prob(Omnibus):                  0.698   Jarque-Bera (JB):                0.574
Skew:                          -0.185   Prob(JB):                        0.750
Kurtosis:                       2.981   Cond. No.                         12.5
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</code></pre>
<p><b>Alternative solution</b></p>
<p>This is an alternative solution using Scikit instead of StatsModel.</p>
<pre><code class="python">#create model
lr = LinearRegression()

#fit model
mod = lr.fit(X,y)

#get coefficients
mod.coef_
</code></pre>

<pre><code>array([ 0.        ,  2.0758066 ,  0.75840009])
</code></pre>
<h1 id="d">(d)</h1>
<pre><code class="python">X = pd.DataFrame({'x1':x1})
X = sm.add_constant(X)

model = sm.OLS(y, X)
results = model.fit()
print(results.summary())
</code></pre>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.439
Model:                            OLS   Adj. R-squared:                  0.433
Method:                 Least Squares   F-statistic:                     76.72
Date:                Thu, 30 Nov 2017   Prob (F-statistic):           5.93e-14
Time:                        20:25:39   Log-Likelihood:                -124.11
No. Observations:                 100   AIC:                             252.2
Df Residuals:                      98   BIC:                             257.4
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          1.8229      0.161     11.295      0.000       1.503       2.143
x1             2.4468      0.279      8.759      0.000       1.892       3.001
==============================================================================
Omnibus:                        0.357   Durbin-Watson:                   1.986
Prob(Omnibus):                  0.836   Jarque-Bera (JB):                0.272
Skew:                          -0.127   Prob(JB):                        0.873
Kurtosis:                       2.963   Cond. No.                         4.17
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</code></pre>
<p>The coefficient value <b>increased</b> to 2.4468 and the <b>null hypothesis can be rejected and the alternative hypothesis accepted</b> because p-value is zero. It can be said that this results are in line with our expectations from (c).</p>
<h1 id="e">(e)</h1>
<pre><code class="python">X = pd.DataFrame({'x2':x2})
X = sm.add_constant(X)

model = sm.OLS(y, X)
results = model.fit()
print(results.summary())
</code></pre>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.340
Model:                            OLS   Adj. R-squared:                  0.333
Method:                 Least Squares   F-statistic:                     50.53
Date:                Thu, 30 Nov 2017   Prob (F-statistic):           1.92e-10
Time:                        20:25:40   Log-Likelihood:                -132.23
No. Observations:                 100   AIC:                             268.5
Df Residuals:                      98   BIC:                             273.7
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          2.1250      0.157     13.572      0.000       1.814       2.436
x2             3.6070      0.507      7.108      0.000       2.600       4.614
==============================================================================
Omnibus:                        1.537   Durbin-Watson:                   1.828
Prob(Omnibus):                  0.464   Jarque-Bera (JB):                1.597
Skew:                          -0.272   Prob(JB):                        0.450
Kurtosis:                       2.704   Cond. No.                         5.89
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</code></pre>
<p>The coefficient value <b>increased</b> to 3.6070 and the <b>null hypothesis can be rejected and the alternative hypothesis accepted</b> because p-value is zero.</p>
<p>These results are significantly different from (c). In (c) the coefficient associated with <span>\(x_2\)</span> had a lower value and the p-value suggested that the null hypothesis couldn't be rejected (coefficient value could be zero). Now, the coefficient value is higher (even higher than the coefficient value resulting from the case where only <span>\(x_1\)</span> is used) and the null hypothesis can be rejected and the alternative hypothesis accepted.</p>
<h1 id="f">(f)</h1>
<p>The results <b>do not contradict</b>.
What's happening here is a <b>collinearity</b> phenomenon.
As suggested by the high correlation values and by the scatter plot (and, of course, from the generation of Y), we can linearly predict <span>\(x_1\)</span> from <span>\(x_2\)</span> (and vice-versa) with a substantial degree of accuracy. This is a clue of collinearity that is confirmed by the regression model. When both variables are combined in the same linear model, one of them loses explanatory power because the variance it explains is already being explained by the other variable. Accordingly, if considered individually, both variables lead to the rejection of the null hypothesis but, if considered together, one of the variables is dismissable.</p>
<p>Finally, the values of the coefficients agree what we know from the underlying model. If one writes <span>\(X2\)</span> in terms of <span>\(X1\)</span>, substitutes it in the model and adds both coefficients of <span>\(X1\)</span>, we get 2.15. This value is well within the confidence interval calculated in (d), namely [1.892; 3.001]. Likewise, for <span>\(X2\)</span> the expected value of the coefficient is 4.3 which is inside the [2.600; 4.614] interval calculated in (e).</p>
<h1 id="g">(g)</h1>
<pre><code class="python"># add observation 
x1 = np.append(x1, 0.1) #to x1
x2 = np.append(x2, 0.8) #to x2
y = np.append(y, 6) #to y

# add to dataframe (easier for outlier analysis plots)
sample = {'x1': .1, 'x2': .8, 'y': 6} #create point
df = df.append(sample, ignore_index=True) #append sample to existing dataframe
</code></pre>

<h3 id="models-analysis">Models analysis</h3>
<pre><code class="python"># model (c)
X = pd.DataFrame({'x1':x1, 'x2':x2})
X = sm.add_constant(X) # no constant is added by the model unless we're using formulas, so we have to add it

model = sm.OLS(y, X)
results = model.fit()

print(results.summary())
</code></pre>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.425
Model:                            OLS   Adj. R-squared:                  0.414
Method:                 Least Squares   F-statistic:                     36.26
Date:                Thu, 30 Nov 2017   Prob (F-statistic):           1.64e-12
Time:                        20:26:20   Log-Likelihood:                -129.50
No. Observations:                 101   AIC:                             265.0
Df Residuals:                      98   BIC:                             272.8
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          1.8697      0.168     11.111      0.000       1.536       2.204
x1             1.2421      0.432      2.876      0.005       0.385       2.099
x2             2.2711      0.698      3.254      0.002       0.886       3.656
==============================================================================
Omnibus:                        0.673   Durbin-Watson:                   1.803
Prob(Omnibus):                  0.714   Jarque-Bera (JB):                0.465
Skew:                          -0.165   Prob(JB):                        0.792
Kurtosis:                       3.035   Cond. No.                         10.2
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</code></pre>
<pre><code class="python"># model (d)
X = pd.DataFrame({'x1':x1})
X = sm.add_constant(X) #no constant is added by the model unless we're using formulas, so we have to add it

model = sm.OLS(y, X)
results = model.fit()

print(results.summary())
</code></pre>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.363
Model:                            OLS   Adj. R-squared:                  0.357
Method:                 Least Squares   F-statistic:                     56.46
Date:                Thu, 30 Nov 2017   Prob (F-statistic):           2.60e-11
Time:                        20:26:25   Log-Likelihood:                -134.68
No. Observations:                 101   AIC:                             273.4
Df Residuals:                      99   BIC:                             278.6
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          1.9419      0.175     11.116      0.000       1.595       2.288
x1             2.2829      0.304      7.514      0.000       1.680       2.886
==============================================================================
Omnibus:                       12.832   Durbin-Watson:                   1.773
Prob(Omnibus):                  0.002   Jarque-Bera (JB):               21.470
Skew:                           0.522   Prob(JB):                     2.18e-05
Kurtosis:                       5.003   Cond. No.                         4.14
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</code></pre>
<pre><code class="python"># model (e)
X = pd.DataFrame({'x2':x2})
X = sm.add_constant(X) #no constant is added by the model unless we're using formulas, so we have to add it

model = sm.OLS(y, X)
results = model.fit()

print(results.summary())
</code></pre>

<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.377
Model:                            OLS   Adj. R-squared:                  0.370
Method:                 Least Squares   F-statistic:                     59.84
Date:                Thu, 30 Nov 2017   Prob (F-statistic):           8.79e-12
Time:                        20:26:30   Log-Likelihood:                -133.59
No. Observations:                 101   AIC:                             271.2
Df Residuals:                      99   BIC:                             276.4
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          2.0962      0.154     13.604      0.000       1.790       2.402
x2             3.7581      0.486      7.736      0.000       2.794       4.722
==============================================================================
Omnibus:                        1.784   Durbin-Watson:                   1.803
Prob(Omnibus):                  0.410   Jarque-Bera (JB):                1.826
Skew:                          -0.299   Prob(JB):                        0.401
Kurtosis:                       2.726   Cond. No.                         5.68
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</code></pre>
<p>Effect on models:
<em> <b>Model (c)</b>. R-squared decreased, which means that the prediction capacity of the model was reduced. The value of the regression coefficients changed: x1 coefficient decreased and x2 coefficient increased. As a consequence, x2 became the coefficient with higher value. The null hypothesis is now rejected in both variables.
</em> <b>Model (d)</b>. The only significant change was the reduction of R-squared.
* <b>Model (e)</b>. The only significant change was a small increase of R-squared.</p>
<h3 id="outliers-and-high-leverage-points-analysis">Outliers and high leverage points analysis</h3>
<p><b>Outliers</b></p>
<p>An outlier is a point for which <span>\(y_i\)</span> is far from the expected range predicted by the fit of the model. This raises the question of whether it is representative of the population.</p>
<p>Outliers can be identified from a univariate, bivariate, or multivariate perspective based on the number of variables (characteristics) considered. The researcher should utilize as many of these perspectives as possible, looking for a consistent pattern across perspectives to identify outliers.</p>
<p>Cases that fall markedly outside the range of the other observations will be seen as isolated points in the scatterplot. A drawback of the bivariate method in general is the potentially large number of scatterplots that arise as the number of variables increases. For three variables, it is only three graphs for all pairwise comparisons. But for five variables, it takes 10 graphs, and for 10 variables it takes 45 scatterplots! Since this analysis doesn't involve more than two variables, we can perform all pairwise comparisons.</p>
<p><b>High leverage points</b></p>
<p>We just saw that outliers are observations for which the response yi is unusual given the predictor xi. In contrast, observations with high leverage have an unusual value for xi. In statistics and in particular in regression analysis, leverage is a measure of how far away the independent variable values of an observation are from those of the other observations.</p>
<p>In a simple linear regression, high leverage observations are fairly easy to
identify, since we can simply look for observations for which the predictor
value is outside of the normal range of the observations. With a single predictor, an extreme x value is simply one that is particularly high or low. </p>
<p>But in a multiple linear regression with many predictors, it is possible to have an observation that is well within the range of each individual predictorâ€™s values, but that is unusual in terms of the full set of predictors. With multiple predictors, extreme x values may be particularly high or low for one or more predictors, or may be "unusual" combinations of predictor values (e.g., with two predictors that are positively correlated, an unusual combination of predictor values might be a high value of one predictor paired with a low value of the other predictor).</p>
<pre><code class="python"># bivariate analysis (x1,x2)
sample = df.iloc[-1:] #to get the last observation
other = df.iloc[:-1] #to get all the observations but the last
ax = other.plot(kind='scatter',x='x1',y='x2', color='blue'); #plot all observations but the last in blue
sample.plot(ax=ax, kind='scatter',x='x1',y='x2', color='red'); #plot last observation added in red
</code></pre>

<p><img alt="png" src="../03_14_files/03_14_35_0.png" /></p>
<ul>
<li>There is an unusual combination of predictor values, so it looks like an high leverage point.</li>
</ul>
<p><b>Note:</b> we are not comparing predictors with responses nor evaluating if yi is far from the value predicted by the model. Therefore, it doesn't make sense to discuss if it's an outlier or not based on this plot.</p>
<pre><code class="python"># bivariate analysis (x1,y)
sample = df.iloc[-1:] # to get the last observation
other = df.iloc[:-1] # to get all the observations but the last
ax = other.plot(kind='scatter',x='x1',y='y', color='blue'); # plot all observations but the last in blue
sample.plot(ax=ax, kind='scatter',x='x1',y='y', color='red'); # plot last observation added in red
</code></pre>

<p><img alt="png" src="../03_14_files/03_14_37_0.png" /></p>
<ul>
<li>The red point does not follow the trend, so it looks like an outlier.</li>
<li>The red point doesn't have an unusual x1 value, so it doesn't look like an high leverage point.</li>
</ul>
<pre><code class="python"># bivariate analysis (x2,y)
sample = df.iloc[-1:] # to get the last observation
other = df.iloc[:-1] # to get all the observations but the last
ax = other.plot(kind='scatter',x='x2',y='y', color='blue'); # plot all observations but the last in blue
sample.plot(ax=ax, kind='scatter',x='x2',y='y', color='red'); # plot last observation added in red
</code></pre>

<p><img alt="png" src="../03_14_files/03_14_39_0.png" /></p>
<ul>
<li>The red point follows the trend, so it doesn't look like an outlier.</li>
<li>The red point has an extreme x2 value, so it looks like an high leverage point.</li>
</ul>
<p><b>In summary:</b> the observation added influences significantly the model, in particular if we consider the regression model that includes x1 and x2. In this case, x2 passed from a neglected variable to a significant variable. This means that even being just 1  observation in 100, this observation reduced the existing phenomenon of collinearity. Also, the R-squared of the model reduced, which signifies a decrease in the model predicition capacity.</p>
<p>According to the scatter plots, the observation added seems to be both an outlier and and an high leverage point. This conclusion can be taken from the visual observation of the observation added when confronted with the remaining observations. The added observations shows an unusual combination of predictor values, extreme predictor values and a substantial different behaviour when compared with other observations in several cases</p>
<h2 id="references">References</h2>
<ul>
<li>https://onlinecourses.science.psu.edu/stat501/node/337</li>
<li>Hair, J. F., Black, B., Babin, B., Anderson, R. E., &amp; Tatham, R. L. (2010). <a href="https://smile.amazon.com/Multivariate-Data-Analysis-Joseph-Hair/dp/0138132631?sa-no-redirect=1">Multivariate Data Analysis</a> (7th
ed.). Upper Saddle River, NJ: Prentice-Hall</li>
</ul></div>
        
    </div>

    <footer class="col-md-12 text-center">
        <hr>
        <p>
        <small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p></small>
    </footer>

    <script src="../../../js/jquery-1.10.2.min.js"></script>
    <script src="../../../js/bootstrap-3.0.3.min.js"></script>
    <script src="../../../js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script>
    var base_url = '../../..';
    </script>
    <script data-main="../../../mkdocs/js/search.js" src="../../../mkdocs/js/require.js"></script>
    <script src="../../../js/base.js"></script>
    <script src="../../../mathjax-config.js"></script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal">
                        <span aria-hidden="true">&times;</span>
                        <span class="sr-only">Close</span>
                    </button>
                    <h4 class="modal-title" id="exampleModalLabel">Search</h4>
                </div>
                <div class="modal-body">
                    <p>
                        From here you can search these documents. Enter your search terms below.
                    </p>
                    <form role="form">
                        <div class="form-group">
                            <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                        </div>
                    </form>
                    <div id="mkdocs-search-results"></div>
                </div>
                <div class="modal-footer">
                </div>
            </div>
        </div>
    </div>

    </body>

</html>
